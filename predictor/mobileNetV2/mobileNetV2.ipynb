{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmppjtknaw_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmppjtknaw_/assets\n",
      "2024-10-02 12:50:15.417709: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-02 12:50:15.417824: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-10-02 12:50:15.767509: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-02 12:50:15.767591: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "WARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. The old converter is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted to mobilenetv2_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Load a pre-trained MobileNetV2 model from tf.keras.applications\n",
    "model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Convert the Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Optional: Disable experimental features to avoid MLIR errors\n",
    "converter.experimental_new_converter = False  # Use the old converter for stability\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Step 3: Save the converted .tflite model\n",
    "tflite_model_path = \"mobilenetv2_model.tflite\"\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model successfully converted to {tflite_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'input_1', 'index': 1, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([  1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'Identity', 'index': 0, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([   1, 1000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load and inspect the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"mobilenetv2_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"Input details: {input_details}\")\n",
    "print(f\"Output details: {output_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Data: [[5.32124133e-04 4.11124813e-04 1.55205710e-03 9.78591619e-04\n",
      "  7.98049557e-04 4.80113726e-04 3.77060380e-03 1.29565757e-04\n",
      "  3.73081217e-04 2.70167511e-04 4.24647878e-04 5.69683325e-05\n",
      "  8.29708442e-05 3.74735449e-04 1.16127703e-04 9.86763116e-05\n",
      "  2.04535885e-04 1.33691428e-04 1.89344515e-04 7.75614317e-05\n",
      "  6.36670855e-04 1.75372767e-03 3.36875004e-04 7.63901975e-04\n",
      "  3.63954430e-04 8.05532734e-04 1.07493310e-03 7.84753996e-04\n",
      "  6.43054082e-04 2.64178670e-04 3.36778932e-04 3.47278983e-04\n",
      "  4.99603921e-04 3.81212994e-05 9.84676648e-04 4.20603901e-05\n",
      "  9.80644545e-05 4.34587710e-05 1.91896717e-04 1.98271140e-04\n",
      "  2.35961037e-04 6.11893949e-04 2.10698548e-04 1.45547136e-04\n",
      "  2.37128901e-04 2.51650956e-04 3.08292016e-04 2.12225554e-04\n",
      "  1.56842405e-04 8.17754990e-05 8.57819978e-05 3.71271599e-05\n",
      "  3.95701383e-04 4.54869762e-04 3.19997373e-04 1.11554109e-04\n",
      "  1.14408460e-04 5.19055640e-04 9.19196114e-04 1.94342414e-04\n",
      "  4.95813329e-05 4.26325947e-04 5.47912146e-04 5.92856726e-04\n",
      "  2.92119366e-04 6.65727246e-04 7.22871686e-04 1.74415283e-04\n",
      "  1.02519733e-03 2.32648221e-04 4.89261292e-04 8.99020699e-04\n",
      "  3.92171816e-04 8.86215363e-04 3.90181027e-04 1.01811113e-03\n",
      "  3.06015980e-04 6.11461175e-04 1.05439534e-03 4.03263519e-04\n",
      "  6.09219889e-04 1.99967669e-03 2.95835605e-04 6.64079271e-04\n",
      "  1.11938360e-04 6.58038698e-05 1.83080250e-04 1.09115710e-04\n",
      "  9.02115935e-05 1.95498011e-04 1.05560721e-04 1.85990371e-04\n",
      "  4.26938699e-04 7.25941136e-05 5.16596810e-05 1.27141670e-04\n",
      "  7.24510828e-05 5.46075898e-05 3.37282923e-04 3.17975122e-04\n",
      "  1.98264912e-04 5.33248880e-04 4.18704731e-04 1.20135970e-04\n",
      "  2.14471482e-04 4.99851703e-05 2.94725935e-04 5.67470770e-03\n",
      "  8.32592894e-04 5.34922001e-04 2.31529586e-03 5.87540772e-03\n",
      "  2.04582117e-04 2.96210492e-04 1.10145961e-03 1.36021769e-03\n",
      "  2.31894097e-04 5.68165269e-04 5.52503392e-04 1.98855036e-04\n",
      "  2.83918896e-04 1.37829906e-04 5.08052821e-04 1.15330738e-04\n",
      "  7.97359564e-04 9.30928873e-05 2.73603597e-04 5.70464588e-04\n",
      "  2.10569793e-04 6.30551949e-04 2.22690214e-04 6.87707216e-05\n",
      "  5.10938989e-04 2.87220289e-04 1.92728286e-04 9.16483477e-05\n",
      "  9.30695896e-05 7.11544490e-05 2.96264596e-04 1.33099689e-04\n",
      "  1.79881477e-04 7.14407710e-04 4.08696069e-04 3.00404936e-04\n",
      "  3.80946789e-04 3.37168254e-04 1.97418989e-03 7.25071703e-04\n",
      "  9.13361146e-04 3.25825834e-03 3.63414263e-04 2.01371950e-04\n",
      "  3.20906402e-04 8.36819876e-04 5.87986200e-04 4.10527922e-04\n",
      "  2.86629132e-04 2.86497525e-04 1.40937336e-04 3.22965119e-04\n",
      "  4.53389403e-05 3.54800839e-04 2.94426456e-04 2.38979395e-04\n",
      "  5.15253632e-04 9.28811642e-05 1.40469318e-04 1.68344995e-04\n",
      "  1.78889924e-04 2.07514866e-04 3.14701902e-05 5.28900826e-04\n",
      "  4.78560833e-04 4.93020285e-04 2.03694173e-04 6.71194648e-05\n",
      "  3.82625993e-04 8.55791950e-05 2.61076260e-04 3.29672650e-04\n",
      "  1.63344390e-04 1.35395225e-04 2.81888933e-04 2.15625798e-04\n",
      "  3.48670932e-04 3.83856794e-04 9.40637110e-05 3.01751134e-04\n",
      "  1.01664846e-04 5.22379851e-05 1.54875786e-04 1.26936735e-04\n",
      "  2.17024644e-04 6.73509712e-05 1.95940855e-04 2.67035910e-04\n",
      "  1.49601037e-04 1.05168743e-04 1.00471691e-04 4.06570005e-04\n",
      "  8.14262530e-05 1.41897108e-04 6.12504082e-05 4.15862974e-04\n",
      "  4.46709950e-04 1.57180344e-04 2.23198469e-04 1.68197919e-04\n",
      "  1.27908643e-04 4.33629146e-04 1.93097148e-04 6.72395923e-04\n",
      "  4.09146276e-04 1.45228041e-04 3.29231872e-04 6.20186504e-04\n",
      "  4.80752904e-04 6.16798352e-04 2.01233925e-04 2.08889585e-04\n",
      "  1.73108652e-04 2.37930028e-04 1.65542893e-04 9.31322284e-04\n",
      "  2.04715907e-04 3.44796194e-04 1.42320569e-04 2.58867338e-04\n",
      "  1.48102918e-04 1.28990432e-04 2.20563918e-04 6.59978541e-05\n",
      "  8.93348697e-05 9.50458780e-05 1.78976959e-04 2.73672631e-04\n",
      "  1.96420733e-04 1.26679632e-04 1.50566091e-04 3.65946296e-04\n",
      "  2.21570212e-04 1.16736242e-04 6.48203364e-04 2.11538529e-04\n",
      "  3.19814171e-05 1.22359651e-03 1.51961343e-04 4.69001505e-04\n",
      "  8.26792209e-04 2.93495163e-04 1.81388896e-04 5.89625619e-04\n",
      "  5.01947827e-04 1.61938748e-04 3.20146792e-04 1.66628815e-04\n",
      "  4.07260173e-04 5.02326351e-04 8.04298790e-04 5.84362424e-04\n",
      "  3.41773353e-04 1.52842767e-04 2.51006451e-04 3.11537820e-04\n",
      "  3.79698526e-04 1.29707681e-04 1.52788474e-04 3.42199608e-04\n",
      "  1.39594660e-04 3.02064553e-04 7.12424167e-04 2.20799528e-04\n",
      "  4.36627277e-04 9.09777707e-04 4.08194726e-04 5.29378711e-04\n",
      "  4.82406031e-04 4.54710156e-04 5.40093693e-04 1.44267746e-03\n",
      "  1.82230619e-03 5.13913867e-04 2.96088052e-04 2.17836560e-03\n",
      "  5.18270419e-04 2.39041052e-04 3.81590362e-04 7.08258653e-04\n",
      "  2.29033059e-04 3.95691924e-04 3.72340495e-04 1.55339963e-04\n",
      "  2.12833605e-04 6.38015918e-04 9.66784894e-04 1.59587595e-04\n",
      "  1.89157284e-03 2.55775201e-04 5.97153965e-04 6.35711651e-04\n",
      "  2.22047005e-04 2.64869625e-04 2.30036821e-04 7.22613186e-05\n",
      "  2.36204534e-04 4.59403032e-04 1.19381082e-04 7.29149091e-04\n",
      "  2.46416952e-04 1.48144245e-04 1.55187142e-03 3.99395620e-04\n",
      "  3.02527100e-04 8.48264972e-05 1.33209018e-04 2.54119834e-04\n",
      "  8.76057238e-05 7.63138451e-05 8.23274371e-04 6.27025001e-05\n",
      "  1.53676083e-04 8.86302805e-05 6.44884494e-05 1.69927676e-04\n",
      "  1.21999547e-04 4.89798149e-05 1.48995503e-04 1.76650158e-03\n",
      "  7.49162864e-04 3.76948010e-04 8.83389730e-04 6.54664240e-04\n",
      "  3.24749365e-03 2.46511685e-04 2.92742945e-04 6.58267818e-04\n",
      "  4.87662968e-04 1.90035847e-04 1.39090887e-04 2.69354321e-04\n",
      "  3.46251007e-04 1.47051949e-04 9.22123800e-05 2.05821445e-04\n",
      "  1.39427095e-04 7.40076939e-04 2.48595112e-04 7.91442348e-04\n",
      "  1.07743836e-03 2.24968651e-04 6.68146706e-04 8.06683151e-04\n",
      "  1.66349870e-03 1.58378237e-03 4.88131540e-04 7.33851994e-05\n",
      "  2.89261749e-04 2.58882763e-04 8.71318800e-04 3.11412761e-04\n",
      "  1.21762198e-04 3.65282613e-04 4.31300752e-04 3.80634825e-04\n",
      "  2.01958333e-04 1.78420029e-04 2.68108910e-04 3.87390726e-04\n",
      "  1.34710092e-04 2.40983994e-04 3.61562532e-04 7.98502180e-04\n",
      "  1.58315248e-04 1.25444538e-04 9.11251700e-05 2.03210555e-04\n",
      "  1.19047123e-04 8.83392291e-04 1.72251181e-04 9.75718576e-05\n",
      "  2.11342325e-04 1.46407270e-04 1.12898415e-04 3.74041352e-04\n",
      "  2.37344517e-04 1.64062294e-04 2.00164941e-04 2.16627406e-04\n",
      "  3.01423250e-04 6.00427506e-04 4.89253551e-04 6.84301660e-04\n",
      "  3.33281787e-04 1.26910832e-04 2.43924762e-04 5.09550970e-04\n",
      "  8.54726750e-05 4.23600868e-05 2.21612063e-04 1.07597047e-03\n",
      "  4.20840253e-04 5.49117394e-04 4.61945019e-04 6.82633370e-04\n",
      "  1.23124793e-02 1.61835309e-02 1.09792105e-04 3.15379410e-04\n",
      "  1.43476544e-04 3.44746467e-03 5.72221295e-04 7.01104436e-05\n",
      "  2.98294355e-04 4.95084969e-04 4.56930211e-05 5.56136729e-05\n",
      "  1.08571585e-04 3.02949082e-02 8.92288590e-05 1.65733552e-04\n",
      "  1.52217704e-04 8.38365871e-04 1.79885261e-04 8.92776006e-04\n",
      "  1.17011892e-03 5.30656008e-03 1.41411467e-04 1.22328915e-04\n",
      "  3.31450370e-04 2.57714791e-03 8.48173513e-04 3.13939148e-04\n",
      "  2.77368643e-04 7.71158724e-04 2.80108739e-04 5.38664986e-04\n",
      "  5.67874580e-04 5.50580909e-03 2.59002787e-04 2.68015341e-04\n",
      "  7.78703077e-04 1.08420255e-03 2.08633734e-04 8.21358524e-04\n",
      "  2.76628300e-04 4.04604274e-04 1.89352810e-04 9.95342038e-04\n",
      "  7.65967881e-04 3.54444725e-04 9.44692001e-05 7.45266734e-05\n",
      "  2.09696154e-04 1.21282465e-04 6.28259877e-05 2.84311915e-04\n",
      "  1.11653040e-04 4.28326923e-04 8.11213322e-05 2.09091071e-04\n",
      "  1.37872633e-03 3.43497319e-04 1.11098425e-03 2.27933211e-04\n",
      "  1.18943739e-04 2.06521698e-04 2.78759515e-04 1.37980503e-04\n",
      "  1.85534556e-03 4.66122787e-04 3.54189746e-04 9.13922209e-04\n",
      "  6.03268680e-04 2.67602358e-04 4.74601875e-05 1.56204111e-03\n",
      "  2.74855498e-04 8.90795636e-05 1.49858359e-04 6.33470016e-04\n",
      "  3.38674756e-04 1.71230626e-04 1.61505348e-04 6.26386784e-04\n",
      "  2.14897655e-03 2.40364301e-04 3.21309868e-04 3.73794581e-04\n",
      "  9.27456538e-04 5.67850191e-04 1.98214984e-04 4.78959468e-04\n",
      "  6.99492637e-04 4.52077133e-04 4.46627673e-05 6.24047010e-04\n",
      "  1.64920770e-04 6.90727727e-04 2.31785321e-04 8.49655014e-04\n",
      "  6.37577963e-04 3.80544108e-04 1.06310545e-04 1.10431465e-04\n",
      "  6.82472237e-05 3.91017442e-04 1.21057814e-03 1.15584153e-04\n",
      "  1.30325920e-04 1.00458281e-04 6.49814087e-04 5.91569056e-04\n",
      "  5.19218331e-04 8.52339726e-04 5.55157196e-04 2.44156108e-03\n",
      "  1.47609884e-04 3.15463962e-03 2.25265045e-04 1.01793045e-03\n",
      "  1.78814546e-04 1.38580130e-04 3.15926969e-04 1.50265300e-03\n",
      "  4.83956363e-04 6.16653473e-04 1.76418762e-04 2.12456915e-04\n",
      "  1.20745011e-04 2.04920507e-04 4.25969542e-04 2.96024402e-04\n",
      "  5.99620980e-04 1.25310980e-04 1.52414036e-03 1.95845001e-04\n",
      "  2.36749975e-03 1.30516346e-04 1.04992464e-03 1.11920737e-04\n",
      "  8.41262471e-03 4.40306583e-04 2.55913532e-04 1.25490071e-04\n",
      "  1.98545225e-04 1.25553692e-03 2.99668143e-04 1.09290413e-04\n",
      "  2.58687796e-04 2.05718633e-03 1.12119553e-04 1.20833538e-04\n",
      "  3.92568094e-04 4.77990718e-04 2.95284437e-03 7.24604120e-04\n",
      "  3.17182858e-03 1.03572626e-02 8.70635486e-05 2.49160134e-04\n",
      "  2.31600163e-04 8.10951402e-04 2.26521632e-04 3.82675789e-05\n",
      "  1.11364432e-04 1.14137700e-04 1.39540760e-04 7.53277200e-05\n",
      "  1.90949999e-04 2.76546023e-04 7.02731835e-04 2.04212047e-04\n",
      "  1.61563425e-04 1.45443692e-03 1.49710318e-02 2.52397964e-04\n",
      "  2.90689262e-04 6.10921124e-04 1.23257545e-04 6.81452802e-04\n",
      "  1.98041831e-04 2.61881214e-04 1.08799362e-03 7.64843717e-04\n",
      "  2.78302505e-05 5.19402267e-04 2.95168109e-04 3.26836482e-04\n",
      "  5.62539673e-04 1.12586393e-04 7.77220703e-05 1.26177759e-03\n",
      "  1.27357183e-04 4.93016967e-04 1.24013211e-04 3.70160386e-04\n",
      "  1.93693710e-03 1.96948284e-04 7.82547431e-05 4.51583968e-04\n",
      "  1.89066748e-03 7.94466469e-04 2.45472882e-04 3.54863092e-04\n",
      "  1.10583799e-03 1.22030739e-04 7.58434762e-05 4.39768803e-04\n",
      "  4.63878416e-04 7.20722252e-04 5.14857180e-04 9.52263799e-05\n",
      "  1.21338919e-04 1.53670364e-04 1.41417855e-04 5.64364251e-04\n",
      "  2.81443819e-04 8.23748007e-04 1.20443350e-04 8.80148087e-04\n",
      "  1.25265520e-04 1.00714690e-03 5.08782803e-04 1.00658035e-04\n",
      "  1.29769920e-04 3.31882417e-04 4.02789621e-04 1.02178099e-04\n",
      "  1.67940278e-03 2.06052419e-03 4.62773547e-04 1.60875177e-04\n",
      "  2.33497890e-03 3.02179222e-04 9.50677786e-04 2.71536788e-04\n",
      "  4.49404033e-04 2.87344126e-04 2.48934026e-04 5.85966278e-04\n",
      "  6.74348790e-04 1.14338596e-04 1.17318043e-04 3.22613108e-04\n",
      "  2.13488494e-03 1.96152111e-03 4.90699010e-03 3.00410378e-04\n",
      "  2.55299499e-04 3.73218325e-03 8.95015430e-04 1.14344817e-04\n",
      "  5.21875452e-04 5.26428455e-04 6.37884543e-04 1.88471837e-04\n",
      "  5.54991246e-04 3.74981225e-03 6.76675991e-05 1.61536547e-04\n",
      "  2.08379724e-03 4.40351752e-04 6.04368222e-04 9.28352019e-05\n",
      "  4.37444804e-04 3.27211834e-04 1.28538959e-04 3.20505904e-04\n",
      "  1.56869093e-04 4.66066092e-04 6.60592108e-04 1.01766933e-03\n",
      "  5.36591001e-03 4.20867174e-04 9.64510182e-05 1.81836178e-04\n",
      "  1.30411703e-04 3.91914928e-03 5.13160427e-04 2.90466734e-04\n",
      "  4.18301002e-04 1.92018560e-04 3.15922755e-03 1.19763904e-03\n",
      "  1.81126325e-05 2.29853700e-04 2.62671587e-04 5.13849664e-05\n",
      "  1.64074343e-04 6.35663106e-04 1.64985729e-04 6.32561496e-05\n",
      "  3.40442057e-04 3.72971670e-04 2.86882068e-03 7.47941027e-04\n",
      "  3.65087937e-04 7.60243784e-05 7.79576891e-04 1.91503292e-04\n",
      "  2.91409553e-04 4.11287807e-02 1.22157886e-04 3.85217689e-04\n",
      "  9.29654459e-04 2.48129829e-04 2.21272989e-04 2.71276018e-04\n",
      "  1.90604944e-03 2.83410172e-05 1.28161206e-04 1.65246674e-04\n",
      "  1.31388661e-04 6.61101134e-04 2.63435126e-04 3.33171687e-04\n",
      "  1.88941276e-03 1.94590946e-04 8.82020511e-04 8.42552181e-05\n",
      "  4.07408748e-04 3.22926615e-04 5.22445131e-04 2.43426985e-04\n",
      "  3.91769223e-04 1.76246380e-04 9.96119852e-05 3.43836378e-04\n",
      "  2.36600544e-03 3.06823698e-04 4.80262999e-04 4.68754239e-04\n",
      "  1.51604749e-04 5.52475220e-03 2.53895618e-04 2.06980432e-04\n",
      "  4.53804096e-04 1.61997520e-03 1.64044061e-04 4.73537220e-04\n",
      "  2.25890079e-04 2.60116067e-04 5.11962106e-04 1.29008316e-04\n",
      "  4.46652109e-03 3.95060168e-04 1.74420960e-02 8.26387026e-04\n",
      "  3.85718013e-05 5.10650862e-04 6.65387663e-04 2.33434868e-04\n",
      "  2.45903531e-04 5.69763768e-04 1.26923303e-04 8.21346603e-03\n",
      "  7.28647501e-05 3.55815690e-04 2.04616459e-03 1.36289003e-04\n",
      "  3.74839612e-04 3.60757607e-04 5.17158769e-04 2.55319843e-04\n",
      "  2.88891228e-04 2.45077797e-04 6.54092000e-05 6.81160382e-05\n",
      "  2.62459880e-03 3.08410672e-04 5.23482275e-04 1.26870465e-04\n",
      "  9.65909450e-04 4.36638948e-04 4.55634799e-05 4.05274710e-04\n",
      "  1.37903274e-03 1.36017741e-04 8.74594698e-05 3.26522131e-04\n",
      "  2.19092774e-03 5.08517667e-04 3.45185254e-04 7.83440133e-04\n",
      "  2.53278093e-04 2.05764140e-04 1.29918510e-04 1.06881096e-04\n",
      "  4.00380086e-04 5.90620039e-04 4.21615150e-05 1.39220268e-03\n",
      "  1.96627178e-03 1.85927769e-04 6.13838842e-04 1.68215984e-03\n",
      "  7.40588468e-04 1.19813823e-03 1.36054397e-04 1.08934270e-04\n",
      "  7.94643347e-05 7.41177762e-04 7.25047488e-04 5.87715185e-04\n",
      "  1.09792731e-04 2.31766189e-03 3.66887543e-04 9.42347106e-04\n",
      "  4.94472508e-04 8.90515294e-05 9.55673284e-04 2.35130909e-04\n",
      "  5.23297535e-03 2.05029093e-04 2.32731560e-04 5.76619583e-04\n",
      "  3.81796926e-05 2.51110148e-04 1.25066424e-03 6.31643750e-04\n",
      "  1.16714649e-03 4.15914517e-04 6.63617684e-04 3.00610700e-04\n",
      "  1.75677254e-04 7.32363900e-04 7.08399108e-04 5.88154478e-04\n",
      "  2.15360473e-04 5.43753733e-04 1.27474937e-04 4.42687975e-04\n",
      "  8.87743430e-04 2.85452465e-04 5.22339775e-04 1.94684399e-04\n",
      "  5.89754665e-04 1.74702879e-03 5.86754468e-04 7.67437625e-04\n",
      "  1.11660920e-04 4.48599254e-04 9.20403516e-04 4.79649665e-04\n",
      "  5.82542911e-04 3.68615496e-04 2.47040851e-04 4.64434648e-04\n",
      "  3.19677551e-04 6.14826859e-05 1.43541081e-04 4.35520633e-04\n",
      "  6.86150917e-04 1.24771264e-04 9.84950806e-04 2.42329552e-04\n",
      "  3.13713070e-04 1.54633657e-04 6.99608063e-04 4.55706904e-05\n",
      "  3.83602426e-04 5.97539358e-04 1.23549823e-03 8.08115350e-04\n",
      "  6.19603845e-04 1.21711812e-04 5.07321965e-04 1.33057858e-03\n",
      "  8.65693510e-05 4.64779790e-04 4.57518618e-04 1.01830182e-03\n",
      "  3.16005619e-03 7.13129004e-04 6.23131520e-04 6.09132752e-04\n",
      "  3.82288970e-04 2.45745177e-04 2.62672227e-04 3.61477747e-03\n",
      "  5.94610930e-04 4.18374228e-04 7.73321663e-05 6.67485700e-04\n",
      "  2.67080351e-04 4.59864875e-03 1.29118722e-04 9.29006492e-05\n",
      "  2.67200195e-03 1.75188965e-04 3.60874576e-04 5.48028947e-05\n",
      "  1.67659106e-04 9.75439965e-04 1.14889597e-04 3.76231782e-03\n",
      "  8.79364437e-04 5.53166086e-04 2.85702990e-03 1.48372448e-04\n",
      "  5.16431825e-03 3.20726802e-04 4.95420070e-04 2.78654654e-04\n",
      "  1.45846861e-03 1.24767632e-03 1.38712217e-04 1.07037683e-03\n",
      "  2.89945751e-02 2.02626412e-04 1.27363863e-04 1.12279516e-03\n",
      "  5.77893748e-04 1.89287122e-03 1.83963659e-03 1.09622302e-03\n",
      "  2.26857155e-04 3.39746795e-04 1.39600917e-04 1.71896315e-03\n",
      "  3.49459075e-03 2.97428400e-04 1.63846053e-04 5.62255700e-05\n",
      "  7.52972701e-05 1.23441656e-04 1.77702153e-04 8.83659013e-05\n",
      "  1.69276987e-04 3.66891414e-04 2.21757829e-04 6.85696607e-04\n",
      "  3.86031170e-05 2.76974373e-04 9.88202519e-05 7.86419492e-04\n",
      "  3.60482460e-04 1.08583903e-04 3.31914867e-04 1.29616790e-04\n",
      "  1.39431271e-04 6.54712319e-04 3.17856437e-04 2.54032115e-04\n",
      "  9.94686343e-05 3.13422381e-04 4.76876070e-04 4.15421207e-04\n",
      "  2.80982553e-04 4.81268245e-04 3.03439883e-04 5.83527901e-04\n",
      "  5.16758708e-04 9.05542693e-05 3.18737031e-04 5.15220163e-05\n",
      "  5.15635547e-05 3.11611948e-04 8.78266525e-03 1.27580992e-04\n",
      "  9.42513725e-05 1.02693368e-04 8.42166992e-05 5.62844434e-05\n",
      "  3.82258906e-04 7.77120586e-05 9.29138099e-04 1.21056662e-04\n",
      "  1.07866118e-03 1.45226382e-04 1.45603521e-02 6.43660547e-03\n",
      "  2.52052676e-03 1.05245039e-03 1.55995399e-01 8.79946980e-04\n",
      "  1.03413616e-03 7.64175830e-03 4.63906303e-03 4.82284231e-03\n",
      "  5.86495772e-02 1.42071187e-03 3.40866827e-04 8.95291159e-04\n",
      "  7.03824684e-03 8.20685935e-04 1.26036248e-04 1.58393552e-04\n",
      "  4.36882401e-04 4.21392295e-04 2.12744708e-04 1.01915048e-03\n",
      "  5.33538056e-04 2.27705619e-04 3.59385216e-04 9.79788718e-04\n",
      "  3.30510200e-04 4.04417166e-04 2.94888450e-04 3.95074836e-04]]\n",
      "Predicted Class Index: 974\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"mobilenetv2_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load a sample image, resize to model input shape, and preprocess\n",
    "img = Image.open(\"sample_image.jpg\").resize((224, 224))\n",
    "input_data = np.expand_dims(np.array(img) / 255.0, axis=0).astype(np.float32)\n",
    "\n",
    "# Set the tensor to point to the input data to be inferred\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output from the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Output Data: {output_data}\")\n",
    "\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class Index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 156\n",
      "Layer 1: input_2 (InputLayer)\n",
      "Layer 2: Conv1 (Conv2D)\n",
      "Layer 3: bn_Conv1 (BatchNormalization)\n",
      "Layer 4: Conv1_relu (ReLU)\n",
      "Layer 5: expanded_conv_depthwise (DepthwiseConv2D)\n",
      "Layer 6: expanded_conv_depthwise_BN (BatchNormalization)\n",
      "Layer 7: expanded_conv_depthwise_relu (ReLU)\n",
      "Layer 8: expanded_conv_project (Conv2D)\n",
      "Layer 9: expanded_conv_project_BN (BatchNormalization)\n",
      "Layer 10: block_1_expand (Conv2D)\n",
      "Layer 11: block_1_expand_BN (BatchNormalization)\n",
      "Layer 12: block_1_expand_relu (ReLU)\n",
      "Layer 13: block_1_pad (ZeroPadding2D)\n",
      "Layer 14: block_1_depthwise (DepthwiseConv2D)\n",
      "Layer 15: block_1_depthwise_BN (BatchNormalization)\n",
      "Layer 16: block_1_depthwise_relu (ReLU)\n",
      "Layer 17: block_1_project (Conv2D)\n",
      "Layer 18: block_1_project_BN (BatchNormalization)\n",
      "Layer 19: block_2_expand (Conv2D)\n",
      "Layer 20: block_2_expand_BN (BatchNormalization)\n",
      "Layer 21: block_2_expand_relu (ReLU)\n",
      "Layer 22: block_2_depthwise (DepthwiseConv2D)\n",
      "Layer 23: block_2_depthwise_BN (BatchNormalization)\n",
      "Layer 24: block_2_depthwise_relu (ReLU)\n",
      "Layer 25: block_2_project (Conv2D)\n",
      "Layer 26: block_2_project_BN (BatchNormalization)\n",
      "Layer 27: block_2_add (Add)\n",
      "Layer 28: block_3_expand (Conv2D)\n",
      "Layer 29: block_3_expand_BN (BatchNormalization)\n",
      "Layer 30: block_3_expand_relu (ReLU)\n",
      "Layer 31: block_3_pad (ZeroPadding2D)\n",
      "Layer 32: block_3_depthwise (DepthwiseConv2D)\n",
      "Layer 33: block_3_depthwise_BN (BatchNormalization)\n",
      "Layer 34: block_3_depthwise_relu (ReLU)\n",
      "Layer 35: block_3_project (Conv2D)\n",
      "Layer 36: block_3_project_BN (BatchNormalization)\n",
      "Layer 37: block_4_expand (Conv2D)\n",
      "Layer 38: block_4_expand_BN (BatchNormalization)\n",
      "Layer 39: block_4_expand_relu (ReLU)\n",
      "Layer 40: block_4_depthwise (DepthwiseConv2D)\n",
      "Layer 41: block_4_depthwise_BN (BatchNormalization)\n",
      "Layer 42: block_4_depthwise_relu (ReLU)\n",
      "Layer 43: block_4_project (Conv2D)\n",
      "Layer 44: block_4_project_BN (BatchNormalization)\n",
      "Layer 45: block_4_add (Add)\n",
      "Layer 46: block_5_expand (Conv2D)\n",
      "Layer 47: block_5_expand_BN (BatchNormalization)\n",
      "Layer 48: block_5_expand_relu (ReLU)\n",
      "Layer 49: block_5_depthwise (DepthwiseConv2D)\n",
      "Layer 50: block_5_depthwise_BN (BatchNormalization)\n",
      "Layer 51: block_5_depthwise_relu (ReLU)\n",
      "Layer 52: block_5_project (Conv2D)\n",
      "Layer 53: block_5_project_BN (BatchNormalization)\n",
      "Layer 54: block_5_add (Add)\n",
      "Layer 55: block_6_expand (Conv2D)\n",
      "Layer 56: block_6_expand_BN (BatchNormalization)\n",
      "Layer 57: block_6_expand_relu (ReLU)\n",
      "Layer 58: block_6_pad (ZeroPadding2D)\n",
      "Layer 59: block_6_depthwise (DepthwiseConv2D)\n",
      "Layer 60: block_6_depthwise_BN (BatchNormalization)\n",
      "Layer 61: block_6_depthwise_relu (ReLU)\n",
      "Layer 62: block_6_project (Conv2D)\n",
      "Layer 63: block_6_project_BN (BatchNormalization)\n",
      "Layer 64: block_7_expand (Conv2D)\n",
      "Layer 65: block_7_expand_BN (BatchNormalization)\n",
      "Layer 66: block_7_expand_relu (ReLU)\n",
      "Layer 67: block_7_depthwise (DepthwiseConv2D)\n",
      "Layer 68: block_7_depthwise_BN (BatchNormalization)\n",
      "Layer 69: block_7_depthwise_relu (ReLU)\n",
      "Layer 70: block_7_project (Conv2D)\n",
      "Layer 71: block_7_project_BN (BatchNormalization)\n",
      "Layer 72: block_7_add (Add)\n",
      "Layer 73: block_8_expand (Conv2D)\n",
      "Layer 74: block_8_expand_BN (BatchNormalization)\n",
      "Layer 75: block_8_expand_relu (ReLU)\n",
      "Layer 76: block_8_depthwise (DepthwiseConv2D)\n",
      "Layer 77: block_8_depthwise_BN (BatchNormalization)\n",
      "Layer 78: block_8_depthwise_relu (ReLU)\n",
      "Layer 79: block_8_project (Conv2D)\n",
      "Layer 80: block_8_project_BN (BatchNormalization)\n",
      "Layer 81: block_8_add (Add)\n",
      "Layer 82: block_9_expand (Conv2D)\n",
      "Layer 83: block_9_expand_BN (BatchNormalization)\n",
      "Layer 84: block_9_expand_relu (ReLU)\n",
      "Layer 85: block_9_depthwise (DepthwiseConv2D)\n",
      "Layer 86: block_9_depthwise_BN (BatchNormalization)\n",
      "Layer 87: block_9_depthwise_relu (ReLU)\n",
      "Layer 88: block_9_project (Conv2D)\n",
      "Layer 89: block_9_project_BN (BatchNormalization)\n",
      "Layer 90: block_9_add (Add)\n",
      "Layer 91: block_10_expand (Conv2D)\n",
      "Layer 92: block_10_expand_BN (BatchNormalization)\n",
      "Layer 93: block_10_expand_relu (ReLU)\n",
      "Layer 94: block_10_depthwise (DepthwiseConv2D)\n",
      "Layer 95: block_10_depthwise_BN (BatchNormalization)\n",
      "Layer 96: block_10_depthwise_relu (ReLU)\n",
      "Layer 97: block_10_project (Conv2D)\n",
      "Layer 98: block_10_project_BN (BatchNormalization)\n",
      "Layer 99: block_11_expand (Conv2D)\n",
      "Layer 100: block_11_expand_BN (BatchNormalization)\n",
      "Layer 101: block_11_expand_relu (ReLU)\n",
      "Layer 102: block_11_depthwise (DepthwiseConv2D)\n",
      "Layer 103: block_11_depthwise_BN (BatchNormalization)\n",
      "Layer 104: block_11_depthwise_relu (ReLU)\n",
      "Layer 105: block_11_project (Conv2D)\n",
      "Layer 106: block_11_project_BN (BatchNormalization)\n",
      "Layer 107: block_11_add (Add)\n",
      "Layer 108: block_12_expand (Conv2D)\n",
      "Layer 109: block_12_expand_BN (BatchNormalization)\n",
      "Layer 110: block_12_expand_relu (ReLU)\n",
      "Layer 111: block_12_depthwise (DepthwiseConv2D)\n",
      "Layer 112: block_12_depthwise_BN (BatchNormalization)\n",
      "Layer 113: block_12_depthwise_relu (ReLU)\n",
      "Layer 114: block_12_project (Conv2D)\n",
      "Layer 115: block_12_project_BN (BatchNormalization)\n",
      "Layer 116: block_12_add (Add)\n",
      "Layer 117: block_13_expand (Conv2D)\n",
      "Layer 118: block_13_expand_BN (BatchNormalization)\n",
      "Layer 119: block_13_expand_relu (ReLU)\n",
      "Layer 120: block_13_pad (ZeroPadding2D)\n",
      "Layer 121: block_13_depthwise (DepthwiseConv2D)\n",
      "Layer 122: block_13_depthwise_BN (BatchNormalization)\n",
      "Layer 123: block_13_depthwise_relu (ReLU)\n",
      "Layer 124: block_13_project (Conv2D)\n",
      "Layer 125: block_13_project_BN (BatchNormalization)\n",
      "Layer 126: block_14_expand (Conv2D)\n",
      "Layer 127: block_14_expand_BN (BatchNormalization)\n",
      "Layer 128: block_14_expand_relu (ReLU)\n",
      "Layer 129: block_14_depthwise (DepthwiseConv2D)\n",
      "Layer 130: block_14_depthwise_BN (BatchNormalization)\n",
      "Layer 131: block_14_depthwise_relu (ReLU)\n",
      "Layer 132: block_14_project (Conv2D)\n",
      "Layer 133: block_14_project_BN (BatchNormalization)\n",
      "Layer 134: block_14_add (Add)\n",
      "Layer 135: block_15_expand (Conv2D)\n",
      "Layer 136: block_15_expand_BN (BatchNormalization)\n",
      "Layer 137: block_15_expand_relu (ReLU)\n",
      "Layer 138: block_15_depthwise (DepthwiseConv2D)\n",
      "Layer 139: block_15_depthwise_BN (BatchNormalization)\n",
      "Layer 140: block_15_depthwise_relu (ReLU)\n",
      "Layer 141: block_15_project (Conv2D)\n",
      "Layer 142: block_15_project_BN (BatchNormalization)\n",
      "Layer 143: block_15_add (Add)\n",
      "Layer 144: block_16_expand (Conv2D)\n",
      "Layer 145: block_16_expand_BN (BatchNormalization)\n",
      "Layer 146: block_16_expand_relu (ReLU)\n",
      "Layer 147: block_16_depthwise (DepthwiseConv2D)\n",
      "Layer 148: block_16_depthwise_BN (BatchNormalization)\n",
      "Layer 149: block_16_depthwise_relu (ReLU)\n",
      "Layer 150: block_16_project (Conv2D)\n",
      "Layer 151: block_16_project_BN (BatchNormalization)\n",
      "Layer 152: Conv_1 (Conv2D)\n",
      "Layer 153: Conv_1_bn (BatchNormalization)\n",
      "Layer 154: out_relu (ReLU)\n",
      "Layer 155: global_average_pooling2d_1 (GlobalAveragePooling2D)\n",
      "Layer 156: predictions (Dense)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained MobileNetV2 model\n",
    "model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Print the total number of layers\n",
    "print(f\"Total number of layers: {len(model.layers)}\")\n",
    "\n",
    "# List all the layer names for detailed inspection\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i+1}: {layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 156\n",
      "Layer 1: input_8 (InputLayer)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 2: Conv1 (Conv2D)\n",
      "  - Weights: 3.38 KB, Activations: 0.00 KB, Total: 3.38 KB\n",
      "\n",
      "Layer 3: bn_Conv1 (BatchNormalization)\n",
      "  - Weights: 0.50 KB, Activations: 0.00 KB, Total: 0.50 KB\n",
      "\n",
      "Layer 4: Conv1_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 5: expanded_conv_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 1.12 KB, Activations: 0.00 KB, Total: 1.12 KB\n",
      "\n",
      "Layer 6: expanded_conv_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 0.50 KB, Activations: 0.00 KB, Total: 0.50 KB\n",
      "\n",
      "Layer 7: expanded_conv_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 8: expanded_conv_project (Conv2D)\n",
      "  - Weights: 2.00 KB, Activations: 0.00 KB, Total: 2.00 KB\n",
      "\n",
      "Layer 9: expanded_conv_project_BN (BatchNormalization)\n",
      "  - Weights: 0.25 KB, Activations: 0.00 KB, Total: 0.25 KB\n",
      "\n",
      "Layer 10: block_1_expand (Conv2D)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 11: block_1_expand_BN (BatchNormalization)\n",
      "  - Weights: 1.50 KB, Activations: 0.00 KB, Total: 1.50 KB\n",
      "\n",
      "Layer 12: block_1_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 13: block_1_pad (ZeroPadding2D)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 14: block_1_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 3.38 KB, Activations: 0.00 KB, Total: 3.38 KB\n",
      "\n",
      "Layer 15: block_1_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 1.50 KB, Activations: 0.00 KB, Total: 1.50 KB\n",
      "\n",
      "Layer 16: block_1_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 17: block_1_project (Conv2D)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 18: block_1_project_BN (BatchNormalization)\n",
      "  - Weights: 0.38 KB, Activations: 0.00 KB, Total: 0.38 KB\n",
      "\n",
      "Layer 19: block_2_expand (Conv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 20: block_2_expand_BN (BatchNormalization)\n",
      "  - Weights: 2.25 KB, Activations: 0.00 KB, Total: 2.25 KB\n",
      "\n",
      "Layer 21: block_2_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 22: block_2_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 5.06 KB, Activations: 0.00 KB, Total: 5.06 KB\n",
      "\n",
      "Layer 23: block_2_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 2.25 KB, Activations: 0.00 KB, Total: 2.25 KB\n",
      "\n",
      "Layer 24: block_2_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 25: block_2_project (Conv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 26: block_2_project_BN (BatchNormalization)\n",
      "  - Weights: 0.38 KB, Activations: 0.00 KB, Total: 0.38 KB\n",
      "\n",
      "Layer 27: block_2_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 28: block_3_expand (Conv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 29: block_3_expand_BN (BatchNormalization)\n",
      "  - Weights: 2.25 KB, Activations: 0.00 KB, Total: 2.25 KB\n",
      "\n",
      "Layer 30: block_3_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 31: block_3_pad (ZeroPadding2D)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 32: block_3_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 5.06 KB, Activations: 0.00 KB, Total: 5.06 KB\n",
      "\n",
      "Layer 33: block_3_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 2.25 KB, Activations: 0.00 KB, Total: 2.25 KB\n",
      "\n",
      "Layer 34: block_3_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 35: block_3_project (Conv2D)\n",
      "  - Weights: 18.00 KB, Activations: 0.00 KB, Total: 18.00 KB\n",
      "\n",
      "Layer 36: block_3_project_BN (BatchNormalization)\n",
      "  - Weights: 0.50 KB, Activations: 0.00 KB, Total: 0.50 KB\n",
      "\n",
      "Layer 37: block_4_expand (Conv2D)\n",
      "  - Weights: 24.00 KB, Activations: 0.00 KB, Total: 24.00 KB\n",
      "\n",
      "Layer 38: block_4_expand_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 39: block_4_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 40: block_4_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 6.75 KB, Activations: 0.00 KB, Total: 6.75 KB\n",
      "\n",
      "Layer 41: block_4_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 42: block_4_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 43: block_4_project (Conv2D)\n",
      "  - Weights: 24.00 KB, Activations: 0.00 KB, Total: 24.00 KB\n",
      "\n",
      "Layer 44: block_4_project_BN (BatchNormalization)\n",
      "  - Weights: 0.50 KB, Activations: 0.00 KB, Total: 0.50 KB\n",
      "\n",
      "Layer 45: block_4_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 46: block_5_expand (Conv2D)\n",
      "  - Weights: 24.00 KB, Activations: 0.00 KB, Total: 24.00 KB\n",
      "\n",
      "Layer 47: block_5_expand_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 48: block_5_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 49: block_5_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 6.75 KB, Activations: 0.00 KB, Total: 6.75 KB\n",
      "\n",
      "Layer 50: block_5_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 51: block_5_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 52: block_5_project (Conv2D)\n",
      "  - Weights: 24.00 KB, Activations: 0.00 KB, Total: 24.00 KB\n",
      "\n",
      "Layer 53: block_5_project_BN (BatchNormalization)\n",
      "  - Weights: 0.50 KB, Activations: 0.00 KB, Total: 0.50 KB\n",
      "\n",
      "Layer 54: block_5_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 55: block_6_expand (Conv2D)\n",
      "  - Weights: 24.00 KB, Activations: 0.00 KB, Total: 24.00 KB\n",
      "\n",
      "Layer 56: block_6_expand_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 57: block_6_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 58: block_6_pad (ZeroPadding2D)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 59: block_6_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 6.75 KB, Activations: 0.00 KB, Total: 6.75 KB\n",
      "\n",
      "Layer 60: block_6_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 3.00 KB, Activations: 0.00 KB, Total: 3.00 KB\n",
      "\n",
      "Layer 61: block_6_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 62: block_6_project (Conv2D)\n",
      "  - Weights: 48.00 KB, Activations: 0.00 KB, Total: 48.00 KB\n",
      "\n",
      "Layer 63: block_6_project_BN (BatchNormalization)\n",
      "  - Weights: 1.00 KB, Activations: 0.00 KB, Total: 1.00 KB\n",
      "\n",
      "Layer 64: block_7_expand (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 65: block_7_expand_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 66: block_7_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 67: block_7_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 68: block_7_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 69: block_7_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 70: block_7_project (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 71: block_7_project_BN (BatchNormalization)\n",
      "  - Weights: 1.00 KB, Activations: 0.00 KB, Total: 1.00 KB\n",
      "\n",
      "Layer 72: block_7_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 73: block_8_expand (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 74: block_8_expand_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 75: block_8_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 76: block_8_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 77: block_8_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 78: block_8_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 79: block_8_project (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 80: block_8_project_BN (BatchNormalization)\n",
      "  - Weights: 1.00 KB, Activations: 0.00 KB, Total: 1.00 KB\n",
      "\n",
      "Layer 81: block_8_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 82: block_9_expand (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 83: block_9_expand_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 84: block_9_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 85: block_9_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 86: block_9_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 87: block_9_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 88: block_9_project (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 89: block_9_project_BN (BatchNormalization)\n",
      "  - Weights: 1.00 KB, Activations: 0.00 KB, Total: 1.00 KB\n",
      "\n",
      "Layer 90: block_9_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 91: block_10_expand (Conv2D)\n",
      "  - Weights: 96.00 KB, Activations: 0.00 KB, Total: 96.00 KB\n",
      "\n",
      "Layer 92: block_10_expand_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 93: block_10_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 94: block_10_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 13.50 KB, Activations: 0.00 KB, Total: 13.50 KB\n",
      "\n",
      "Layer 95: block_10_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 6.00 KB, Activations: 0.00 KB, Total: 6.00 KB\n",
      "\n",
      "Layer 96: block_10_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 97: block_10_project (Conv2D)\n",
      "  - Weights: 144.00 KB, Activations: 0.00 KB, Total: 144.00 KB\n",
      "\n",
      "Layer 98: block_10_project_BN (BatchNormalization)\n",
      "  - Weights: 1.50 KB, Activations: 0.00 KB, Total: 1.50 KB\n",
      "\n",
      "Layer 99: block_11_expand (Conv2D)\n",
      "  - Weights: 216.00 KB, Activations: 0.00 KB, Total: 216.00 KB\n",
      "\n",
      "Layer 100: block_11_expand_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 101: block_11_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 102: block_11_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 20.25 KB, Activations: 0.00 KB, Total: 20.25 KB\n",
      "\n",
      "Layer 103: block_11_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 104: block_11_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 105: block_11_project (Conv2D)\n",
      "  - Weights: 216.00 KB, Activations: 0.00 KB, Total: 216.00 KB\n",
      "\n",
      "Layer 106: block_11_project_BN (BatchNormalization)\n",
      "  - Weights: 1.50 KB, Activations: 0.00 KB, Total: 1.50 KB\n",
      "\n",
      "Layer 107: block_11_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 108: block_12_expand (Conv2D)\n",
      "  - Weights: 216.00 KB, Activations: 0.00 KB, Total: 216.00 KB\n",
      "\n",
      "Layer 109: block_12_expand_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 110: block_12_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 111: block_12_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 20.25 KB, Activations: 0.00 KB, Total: 20.25 KB\n",
      "\n",
      "Layer 112: block_12_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 113: block_12_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 114: block_12_project (Conv2D)\n",
      "  - Weights: 216.00 KB, Activations: 0.00 KB, Total: 216.00 KB\n",
      "\n",
      "Layer 115: block_12_project_BN (BatchNormalization)\n",
      "  - Weights: 1.50 KB, Activations: 0.00 KB, Total: 1.50 KB\n",
      "\n",
      "Layer 116: block_12_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 117: block_13_expand (Conv2D)\n",
      "  - Weights: 216.00 KB, Activations: 0.00 KB, Total: 216.00 KB\n",
      "\n",
      "Layer 118: block_13_expand_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 119: block_13_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 120: block_13_pad (ZeroPadding2D)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 121: block_13_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 20.25 KB, Activations: 0.00 KB, Total: 20.25 KB\n",
      "\n",
      "Layer 122: block_13_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 9.00 KB, Activations: 0.00 KB, Total: 9.00 KB\n",
      "\n",
      "Layer 123: block_13_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 124: block_13_project (Conv2D)\n",
      "  - Weights: 360.00 KB, Activations: 0.00 KB, Total: 360.00 KB\n",
      "\n",
      "Layer 125: block_13_project_BN (BatchNormalization)\n",
      "  - Weights: 2.50 KB, Activations: 0.00 KB, Total: 2.50 KB\n",
      "\n",
      "Layer 126: block_14_expand (Conv2D)\n",
      "  - Weights: 600.00 KB, Activations: 0.00 KB, Total: 600.00 KB\n",
      "\n",
      "Layer 127: block_14_expand_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 128: block_14_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 129: block_14_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 33.75 KB, Activations: 0.00 KB, Total: 33.75 KB\n",
      "\n",
      "Layer 130: block_14_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 131: block_14_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 132: block_14_project (Conv2D)\n",
      "  - Weights: 600.00 KB, Activations: 0.00 KB, Total: 600.00 KB\n",
      "\n",
      "Layer 133: block_14_project_BN (BatchNormalization)\n",
      "  - Weights: 2.50 KB, Activations: 0.00 KB, Total: 2.50 KB\n",
      "\n",
      "Layer 134: block_14_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 135: block_15_expand (Conv2D)\n",
      "  - Weights: 600.00 KB, Activations: 0.00 KB, Total: 600.00 KB\n",
      "\n",
      "Layer 136: block_15_expand_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 137: block_15_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 138: block_15_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 33.75 KB, Activations: 0.00 KB, Total: 33.75 KB\n",
      "\n",
      "Layer 139: block_15_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 140: block_15_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 141: block_15_project (Conv2D)\n",
      "  - Weights: 600.00 KB, Activations: 0.00 KB, Total: 600.00 KB\n",
      "\n",
      "Layer 142: block_15_project_BN (BatchNormalization)\n",
      "  - Weights: 2.50 KB, Activations: 0.00 KB, Total: 2.50 KB\n",
      "\n",
      "Layer 143: block_15_add (Add)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 144: block_16_expand (Conv2D)\n",
      "  - Weights: 600.00 KB, Activations: 0.00 KB, Total: 600.00 KB\n",
      "\n",
      "Layer 145: block_16_expand_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 146: block_16_expand_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 147: block_16_depthwise (DepthwiseConv2D)\n",
      "  - Weights: 33.75 KB, Activations: 0.00 KB, Total: 33.75 KB\n",
      "\n",
      "Layer 148: block_16_depthwise_BN (BatchNormalization)\n",
      "  - Weights: 15.00 KB, Activations: 0.00 KB, Total: 15.00 KB\n",
      "\n",
      "Layer 149: block_16_depthwise_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 150: block_16_project (Conv2D)\n",
      "  - Weights: 1200.00 KB, Activations: 0.00 KB, Total: 1200.00 KB\n",
      "\n",
      "Layer 151: block_16_project_BN (BatchNormalization)\n",
      "  - Weights: 5.00 KB, Activations: 0.00 KB, Total: 5.00 KB\n",
      "\n",
      "Layer 152: Conv_1 (Conv2D)\n",
      "  - Weights: 1600.00 KB, Activations: 0.00 KB, Total: 1600.00 KB\n",
      "\n",
      "Layer 153: Conv_1_bn (BatchNormalization)\n",
      "  - Weights: 20.00 KB, Activations: 0.00 KB, Total: 20.00 KB\n",
      "\n",
      "Layer 154: out_relu (ReLU)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 155: global_average_pooling2d_7 (GlobalAveragePooling2D)\n",
      "  - Weights: 0.00 KB, Activations: 0.00 KB, Total: 0.00 KB\n",
      "\n",
      "Layer 156: predictions (Dense)\n",
      "  - Weights: 5003.91 KB, Activations: 0.00 KB, Total: 5003.91 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained MobileNetV2 model\n",
    "model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Helper function to calculate size in bytes of a tensor\n",
    "def get_tensor_size(tensor):\n",
    "    if tensor is None:\n",
    "        return 0\n",
    "    # Number of elements * size of each element (assuming float32)\n",
    "    return tf.size(tensor).numpy() * tensor.dtype.size\n",
    "\n",
    "# Function to calculate the size of the activations using the output shape\n",
    "def get_activation_size(output_shape, dtype=tf.float32):\n",
    "    # Check if output_shape is valid and not a tuple with None values\n",
    "    if output_shape is None or not isinstance(output_shape, tuple) or None in output_shape:\n",
    "        return 0\n",
    "    try:\n",
    "        # Multiply all dimensions to get the number of elements (ignoring batch size)\n",
    "        num_elements = 1\n",
    "        for dim in output_shape:\n",
    "            if dim is not None:  # Ignore undefined dimensions\n",
    "                num_elements *= dim\n",
    "        return num_elements * dtype.size  # Size of each element (4 bytes for float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating activation size for shape {output_shape}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Display total number of layers in the model\n",
    "print(f\"Total number of layers: {len(model.layers)}\")\n",
    "\n",
    "# Loop through each layer to calculate weight and activation sizes\n",
    "for i, layer in enumerate(model.layers):\n",
    "    weight_size = 0\n",
    "    activation_size = 0\n",
    "\n",
    "    # Calculate the size of weights (weights + biases)\n",
    "    for weight in layer.weights:\n",
    "        weight_size += get_tensor_size(weight)\n",
    "\n",
    "    # Calculate the size of the activations using the output shape\n",
    "    activation_size = get_activation_size(layer.output_shape)\n",
    "\n",
    "    # Ensure `activation_size` is an integer\n",
    "    if not isinstance(activation_size, int):\n",
    "        print(f\"Warning: Activation size for layer {layer.name} is not an integer: {activation_size}\")\n",
    "        activation_size = 0\n",
    "\n",
    "    # Calculate the total size for the layer\n",
    "    total_layer_size = weight_size + activation_size\n",
    "\n",
    "    # Print the layer details along with memory sizes\n",
    "    print(f\"Layer {i+1}: {layer.name} ({layer.__class__.__name__})\")\n",
    "    print(f\"  - Weights: {weight_size / 1024:.2f} KB, Activations: {activation_size / 1024:.2f} KB, Total: {total_layer_size / 1024:.2f} KB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 156\n",
      "Initial Conv Block:\n",
      "  - Weights: 5.00 KB, Activations: 0.00 KB, Total: 5.00 KB\n",
      "\n",
      "Residual Block 1:\n",
      "  - Weights: 15.12 KB, Activations: 0.00 KB, Total: 15.12 KB\n",
      "\n",
      "Residual Block 2:\n",
      "  - Weights: 46.31 KB, Activations: 0.00 KB, Total: 46.31 KB\n",
      "\n",
      "Residual Block 3:\n",
      "  - Weights: 102.31 KB, Activations: 0.00 KB, Total: 102.31 KB\n",
      "\n",
      "Residual Block 4:\n",
      "  - Weights: 98.50 KB, Activations: 0.00 KB, Total: 98.50 KB\n",
      "\n",
      "Bottleneck Block:\n",
      "  - Weights: 170.50 KB, Activations: 0.00 KB, Total: 170.50 KB\n",
      "\n",
      "Final Conv Block:\n",
      "  - Weights: 8362.50 KB, Activations: 0.00 KB, Total: 8362.50 KB\n",
      "\n",
      "Output Layer:\n",
      "  - Weights: 5023.91 KB, Activations: 0.00 KB, Total: 5023.91 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained MobileNetV2 model\n",
    "model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Helper function to calculate size in bytes of a tensor\n",
    "def get_tensor_size(tensor):\n",
    "    if tensor is None:\n",
    "        return 0\n",
    "    # Number of elements * size of each element (assuming float32)\n",
    "    return tf.size(tensor).numpy() * tensor.dtype.size\n",
    "\n",
    "# Function to calculate the size of the activations using the output shape\n",
    "def get_activation_size(output_shape, dtype=tf.float32):\n",
    "    if output_shape is None or not isinstance(output_shape, tuple) or None in output_shape:\n",
    "        return 0\n",
    "    try:\n",
    "        # Multiply all dimensions to get the number of elements (ignoring batch size)\n",
    "        num_elements = 1\n",
    "        for dim in output_shape:\n",
    "            if dim is not None:  # Ignore undefined dimensions\n",
    "                num_elements *= dim\n",
    "        return num_elements * dtype.size  # Size of each element (4 bytes for float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating activation size for shape {output_shape}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Display total number of layers in the model\n",
    "print(f\"Total number of layers: {len(model.layers)}\")\n",
    "\n",
    "# Define functional blocks for coarse-grained breakdown\n",
    "functional_blocks = {\n",
    "    'Initial Conv Block': list(range(0, 5)),      # Conv2D, BatchNorm, ReLU6\n",
    "    'Residual Block 1': list(range(5, 15)),       # Block 1: expand, depthwise, project\n",
    "    'Residual Block 2': list(range(15, 27)),      # Block 2: expand, depthwise, project\n",
    "    'Residual Block 3': list(range(27, 43)),      # Block 3: expand, depthwise, project\n",
    "    'Residual Block 4': list(range(43, 61)),      # Block 4: expand, depthwise, project\n",
    "    'Bottleneck Block': list(range(61, 69)),      # Bottleneck layers\n",
    "    'Final Conv Block': list(range(69, 152)),     # Final convolution + BatchNorm + Activation\n",
    "    'Output Layer': list(range(152, 156))         # GlobalAveragePooling2D + Dense\n",
    "}\n",
    "\n",
    "# Calculate and display memory usage for each functional block\n",
    "for block_name, layer_indices in functional_blocks.items():\n",
    "    block_weight_size = 0\n",
    "    block_activation_size = 0\n",
    "\n",
    "    # Aggregate sizes for all layers in the functional block\n",
    "    for index in layer_indices:\n",
    "        layer = model.layers[index]\n",
    "        # Calculate weight size for the current layer\n",
    "        for weight in layer.weights:\n",
    "            block_weight_size += get_tensor_size(weight)\n",
    "        \n",
    "        # Calculate activation size\n",
    "        activation_size = get_activation_size(layer.output_shape)\n",
    "        if isinstance(activation_size, int):\n",
    "            block_activation_size += activation_size\n",
    "\n",
    "    # Calculate total size for the functional block\n",
    "    block_total_size = block_weight_size + block_activation_size\n",
    "    print(f\"{block_name}:\")\n",
    "    print(f\"  - Weights: {block_weight_size / 1024:.2f} KB, Activations: {block_activation_size / 1024:.2f} KB, Total: {block_total_size / 1024:.2f} KB\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
