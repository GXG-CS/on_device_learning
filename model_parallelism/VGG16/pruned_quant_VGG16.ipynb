{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 7s 2s/step - loss: 48683192.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 3s 2s/step - loss: 2335956271104.0000 - accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpl8m51r1u/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpl8m51r1u/assets\n",
      "/Users/xiaoguang_guo@mines.edu/anaconda3/envs/mcu/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-10-09 21:22:50.970916: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-10-09 21:22:50.970946: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-10-09 21:22:50.971591: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpl8m51r1u\n",
      "2024-10-09 21:22:50.974013: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-10-09 21:22:50.974020: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpl8m51r1u\n",
      "2024-10-09 21:22:50.978793: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-10-09 21:22:50.981153: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-10-09 21:22:51.543498: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpl8m51r1u\n",
      "2024-10-09 21:22:51.564244: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 592654 microseconds.\n",
      "2024-10-09 21:22:51.588264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned and Quantized VGG16 model has been saved as 'pruned_quantized_vgg16.tflite'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the Pre-trained VGG16 Model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Create a New Model with Classification Layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1000, activation='softmax')  # Assuming ImageNet with 1000 classes\n",
    "])\n",
    "\n",
    "# Step 3: Apply Pruning to the Model with High Sparsity (90%)\n",
    "pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.30,  # Start pruning at 30% sparsity\n",
    "    final_sparsity=0.90,    # Increase pruning to 90% sparsity for smaller model\n",
    "    begin_step=0,\n",
    "    end_step=1000\n",
    ")\n",
    "\n",
    "# Wrap the model with pruning\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "    model,\n",
    "    pruning_schedule=pruning_schedule\n",
    ")\n",
    "\n",
    "# Compile the Pruned Model\n",
    "pruned_model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Fine-Tune the Pruned Model (Using Dummy Data for Demo)\n",
    "dummy_data = np.random.rand(10, 224, 224, 3)\n",
    "dummy_labels = np.random.randint(0, 1000, size=(10, 1000))\n",
    "\n",
    "# Define Pruning Callbacks\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),  # Required for pruning\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./pruning_logs')  # Optional: Logs pruning stats\n",
    "]\n",
    "\n",
    "# Train the Model with Pruning Callbacks\n",
    "pruned_model.fit(dummy_data, dummy_labels, epochs=2, batch_size=5, callbacks=callbacks)\n",
    "\n",
    "# Step 5: Strip the Pruning Wrappers for TensorFlow Lite Conversion\n",
    "stripped_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Step 6: Apply Weight Clustering for Further Size Reduction\n",
    "clustering_params = {\n",
    "    'number_of_clusters': 8,\n",
    "    'cluster_centroids_init': tfmot.clustering.keras.CentroidInitialization.KMEANS_PLUS_PLUS\n",
    "}\n",
    "\n",
    "# Apply clustering\n",
    "clustered_model = tfmot.clustering.keras.cluster_weights(stripped_model, **clustering_params)\n",
    "\n",
    "# Recompile the clustered model before fine-tuning\n",
    "clustered_model.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the clustered model (Using the same dummy data)\n",
    "clustered_model.fit(dummy_data, dummy_labels, epochs=2, batch_size=5)\n",
    "\n",
    "# Step 7: Strip Clustering Wrappers Before Conversion\n",
    "final_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n",
    "\n",
    "# Step 8: Define a Representative Dataset for Full Integer Quantization\n",
    "def representative_data_gen():\n",
    "    for _ in range(100):\n",
    "        # Replace with a real sample from the dataset for accurate calibration\n",
    "        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n",
    "\n",
    "# Step 9: Convert to a Full Integer Quantized TFLite Model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # Quantize inputs to int8\n",
    "converter.inference_output_type = tf.int8  # Quantize outputs to int8\n",
    "\n",
    "# Convert the model to TensorFlow Lite\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Step 10: Save the Fully Optimized and Quantized Model as a .tflite File\n",
    "with open(\"optimized_vgg16.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "print(\"Fully optimized and quantized VGG16 model has been saved as 'optimized_vgg16.tflite'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
