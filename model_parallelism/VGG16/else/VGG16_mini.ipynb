{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WNID to Index Mapping (First 10): [('n01440764', 0), ('n01443537', 1), ('n01484850', 2), ('n01491361', 3), ('n01494475', 4), ('n01496331', 5), ('n01498041', 6), ('n01514668', 7), ('n01514859', 8), ('n01518878', 9)]\n",
      "New ground truth file created: /Users/xiaoguang_guo@mines.edu/Documents/projects/datasets/imagenet-mini/imagenet_mini_gt.txt\n",
      "Total labels generated: 3923\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base path based on your directory structure\n",
    "base_dir = \"/Users/xiaoguang_guo@mines.edu/Documents/projects/datasets/imagenet-mini\"\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "class_index_file = os.path.join(base_dir, \"imagenet_class_index.json\")\n",
    "new_ground_truth_file = os.path.join(base_dir, \"imagenet_mini_gt.txt\")\n",
    "\n",
    "# Load the ImageNet class index file\n",
    "with open(class_index_file, 'r') as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "\n",
    "# Create a WNID-to-index mapping (maps WNIDs to numerical indices, e.g., \"n01440764\": 0)\n",
    "wnid_to_index = {v[0]: int(k) for k, v in imagenet_class_index.items()}\n",
    "\n",
    "# Debug: Print the first few mappings to ensure correctness\n",
    "print(f\"WNID to Index Mapping (First 10): {list(wnid_to_index.items())[:10]}\")\n",
    "\n",
    "# Generate the ground truth labels for the ImageNet Mini validation set\n",
    "ground_truth_labels = []\n",
    "image_list = []\n",
    "\n",
    "# Traverse through the validation directory and generate labels based on folder names\n",
    "for folder in sorted(os.listdir(val_dir)):\n",
    "    folder_path = os.path.join(val_dir, folder)\n",
    "    if os.path.isdir(folder_path) and folder in wnid_to_index:\n",
    "        class_label = wnid_to_index[folder]  # Get the numerical label for the folder\n",
    "        for img_name in sorted(os.listdir(folder_path)):\n",
    "            # Record the image name and its corresponding class index\n",
    "            image_list.append(img_name)\n",
    "            ground_truth_labels.append(str(class_label))  # Correct label based on folder WNID\n",
    "\n",
    "# Write the new ground truth file\n",
    "with open(new_ground_truth_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(ground_truth_labels))\n",
    "\n",
    "print(f\"New ground truth file created: {new_ground_truth_file}\")\n",
    "print(f\"Total labels generated: {len(ground_truth_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Ground Truth Labels: [0, 0, 0, 1, 1, 1, 2, 2, 2, 2]\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "Image: ILSVRC2012_val_00009111.JPEG, Top-1 Prediction: 0, Ground Truth: 0\n",
      "Top-5 Predictions: [  0 114 113 758 124]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Image: ILSVRC2012_val_00030740.JPEG, Top-1 Prediction: 389, Ground Truth: 0\n",
      "Top-5 Predictions: [389   0 395 394 391]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00046252.JPEG, Top-1 Prediction: 0, Ground Truth: 0\n",
      "Top-5 Predictions: [  0 390 389 997 114]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "Image: ILSVRC2012_val_00000994.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 393 392   0  29]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "Image: ILSVRC2012_val_00002241.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 971 467 669 880]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00020822.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 392 973 393 108]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00014467.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   6 801 433  33]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00019409.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   3   4  33 148]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00027514.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [2 3 5 6 4]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Image: ILSVRC2012_val_00030127.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   3 391   4 394]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Total Images Evaluated: 50\n",
      "Top-1 Accuracy: 0.7600\n",
      "Top-5 Accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "\n",
    "# Paths to your dataset and necessary files\n",
    "base_dir = \"/Users/xiaoguang_guo@mines.edu/Documents/projects/datasets/imagenet-mini\"\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "ground_truth_file = os.path.join(base_dir, \"imagenet_mini_gt.txt\")\n",
    "class_index_file = os.path.join(base_dir, \"imagenet_class_index.json\")\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# Load ImageNet class index to get the human-readable labels\n",
    "with open(class_index_file, 'r') as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "    index_to_label = {int(key): value[1] for key, value in imagenet_class_index.items()}\n",
    "\n",
    "# Load the new ground truth labels file for ImageNet Mini\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    ground_truth_labels = f.readlines()\n",
    "    ground_truth_labels = [int(label.strip()) for label in ground_truth_labels]\n",
    "\n",
    "# Debug: Print first few ground truth labels to ensure they are correctly loaded\n",
    "print(f\"First 10 Ground Truth Labels: {ground_truth_labels[:10]}\")\n",
    "\n",
    "# Initialize counters for Top-1 and Top-5 accuracy\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total_images = 0\n",
    "\n",
    "# Set the limit for evaluation\n",
    "max_images = 50  # Adjust this to evaluate more or fewer images\n",
    "\n",
    "# Loop through the validation set and evaluate\n",
    "for folder in sorted(os.listdir(val_dir)):\n",
    "    folder_path = os.path.join(val_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img_name in sorted(os.listdir(folder_path)):\n",
    "            if total_images >= max_images:\n",
    "                break\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "\n",
    "            # Run inference using VGG16\n",
    "            preds = model.predict(x)\n",
    "\n",
    "            # Decode the top-5 predictions\n",
    "            top5_preds = np.argsort(preds[0])[-5:][::-1]  # Get top-5 indices\n",
    "            top1_pred = top5_preds[0]  # Top-1 prediction\n",
    "\n",
    "            # Get the corresponding ground truth label\n",
    "            ground_truth_label = ground_truth_labels[total_images]\n",
    "            total_images += 1\n",
    "\n",
    "            # Debug: Print predictions and ground truth for the first few images\n",
    "            if total_images <= 10:\n",
    "                print(f\"Image: {img_name}, Top-1 Prediction: {top1_pred}, Ground Truth: {ground_truth_label}\")\n",
    "                print(f\"Top-5 Predictions: {top5_preds}\")\n",
    "\n",
    "            # Check if the top-1 prediction matches the ground truth\n",
    "            if top1_pred == ground_truth_label:\n",
    "                top1_correct += 1\n",
    "\n",
    "            # Check if ground truth label is in top-5 predictions\n",
    "            if ground_truth_label in top5_preds:\n",
    "                top5_correct += 1\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "top1_accuracy = top1_correct / total_images\n",
    "top5_accuracy = top5_correct / total_images\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Total Images Evaluated: {total_images}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Ground Truth Labels: [0, 0, 0, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ILSVRC2012_val_00009111.JPEG, Top-1 Prediction: 0, Ground Truth: 0\n",
      "Top-5 Predictions: [  0 114 113 758 124]\n",
      "Image: ILSVRC2012_val_00030740.JPEG, Top-1 Prediction: 389, Ground Truth: 0\n",
      "Top-5 Predictions: [389   0 395 394 391]\n",
      "Image: ILSVRC2012_val_00046252.JPEG, Top-1 Prediction: 0, Ground Truth: 0\n",
      "Top-5 Predictions: [  0 390 389 997 114]\n",
      "Image: ILSVRC2012_val_00000994.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 393 392   0  29]\n",
      "Image: ILSVRC2012_val_00002241.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 971 467 669 880]\n",
      "Image: ILSVRC2012_val_00020822.JPEG, Top-1 Prediction: 1, Ground Truth: 1\n",
      "Top-5 Predictions: [  1 392 973 393 108]\n",
      "Image: ILSVRC2012_val_00014467.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   6 801 433  33]\n",
      "Image: ILSVRC2012_val_00019409.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   3   4  33 148]\n",
      "Image: ILSVRC2012_val_00027514.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [2 3 5 6 4]\n",
      "Image: ILSVRC2012_val_00030127.JPEG, Top-1 Prediction: 2, Ground Truth: 2\n",
      "Top-5 Predictions: [  2   3 391   4 394]\n",
      "Total Images Evaluated: 50\n",
      "Top-1 Accuracy: 0.7600\n",
      "Top-5 Accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Paths to your dataset and necessary files\n",
    "base_dir = \"/Users/xiaoguang_guo@mines.edu/Documents/projects/datasets/imagenet-mini\"\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "ground_truth_file = os.path.join(base_dir, \"imagenet_mini_gt.txt\")\n",
    "class_index_file = os.path.join(base_dir, \"imagenet_class_index.json\")\n",
    "\n",
    "# Load ImageNet class index to get the human-readable labels\n",
    "with open(class_index_file, 'r') as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "    index_to_label = {int(key): value[1] for key, value in imagenet_class_index.items()}\n",
    "\n",
    "# Load the new ground truth labels file for ImageNet Mini\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    ground_truth_labels = f.readlines()\n",
    "    ground_truth_labels = [int(label.strip()) for label in ground_truth_labels]\n",
    "\n",
    "# Debug: Print first few ground truth labels to ensure they are correctly loaded\n",
    "print(f\"First 10 Ground Truth Labels: {ground_truth_labels[:10]}\")\n",
    "\n",
    "# Load the TFLite model\n",
    "tflite_model_path = \"VGG16_classification.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details for the interpreter\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize counters for Top-1 and Top-5 accuracy\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total_images = 0\n",
    "\n",
    "# Set the limit for evaluation\n",
    "max_images = 50  # Adjust this to evaluate more or fewer images\n",
    "\n",
    "# Loop through the validation set and evaluate\n",
    "for folder in sorted(os.listdir(val_dir)):\n",
    "    folder_path = os.path.join(val_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img_name in sorted(os.listdir(folder_path)):\n",
    "            if total_images >= max_images:\n",
    "                break\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = x.astype(np.float32)  # Convert to float32 for TFLite\n",
    "            x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "\n",
    "            # Run inference using TFLite interpreter\n",
    "            interpreter.set_tensor(input_details[0]['index'], x)\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Get predictions from the output tensor\n",
    "            preds = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "            # Decode the top-5 predictions\n",
    "            top5_preds = np.argsort(preds)[-5:][::-1]  # Get top-5 indices\n",
    "            top1_pred = top5_preds[0]  # Top-1 prediction\n",
    "\n",
    "            # Get the corresponding ground truth label\n",
    "            ground_truth_label = ground_truth_labels[total_images]\n",
    "            total_images += 1\n",
    "\n",
    "            # Debug: Print predictions and ground truth for the first few images\n",
    "            if total_images <= 10:\n",
    "                print(f\"Image: {img_name}, Top-1 Prediction: {top1_pred}, Ground Truth: {ground_truth_label}\")\n",
    "                print(f\"Top-5 Predictions: {top5_preds}\")\n",
    "\n",
    "            # Check if the top-1 prediction matches the ground truth\n",
    "            if top1_pred == ground_truth_label:\n",
    "                top1_correct += 1\n",
    "\n",
    "            # Check if ground truth label is in top-5 predictions\n",
    "            if ground_truth_label in top5_preds:\n",
    "                top5_correct += 1\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "top1_accuracy = top1_correct / total_images\n",
    "top5_accuracy = top5_correct / total_images\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Total Images Evaluated: {total_images}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " prune_low_magnitude_block1  (None, 224, 224, 64)      3522      \n",
      " _conv1 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block1  (None, 224, 224, 64)      73794     \n",
      " _conv2 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block1  (None, 112, 112, 64)      1         \n",
      " _pool (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_block2  (None, 112, 112, 128)     147586    \n",
      " _conv1 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block2  (None, 112, 112, 128)     295042    \n",
      " _conv2 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block2  (None, 56, 56, 128)       1         \n",
      " _pool (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_block3  (None, 56, 56, 256)       590082    \n",
      " _conv1 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block3  (None, 56, 56, 256)       1179906   \n",
      " _conv2 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block3  (None, 56, 56, 256)       1179906   \n",
      " _conv3 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block3  (None, 28, 28, 256)       1         \n",
      " _pool (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_block4  (None, 28, 28, 512)       2359810   \n",
      " _conv1 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block4  (None, 28, 28, 512)       4719106   \n",
      " _conv2 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block4  (None, 28, 28, 512)       4719106   \n",
      " _conv3 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block4  (None, 14, 14, 512)       1         \n",
      " _pool (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_block5  (None, 14, 14, 512)       4719106   \n",
      " _conv1 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block5  (None, 14, 14, 512)       4719106   \n",
      " _conv2 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block5  (None, 14, 14, 512)       4719106   \n",
      " _conv3 (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_block5  (None, 7, 7, 512)         1         \n",
      " _pool (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 25088)             1         \n",
      " n (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_fc1 (P  (None, 4096)              205524994 \n",
      " runeLowMagnitude)                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_fc2 (P  (None, 4096)              33558530  \n",
      " runeLowMagnitude)                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_predic  (None, 1000)              8193002   \n",
      " tions (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276701710 (1.03 GB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 138344166 (527.74 MB)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped pruned model saved as vgg16_pruned_stripped.h5\n",
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpkahogt2a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpkahogt2a/assets\n",
      "/Users/xiaoguang_guo@mines.edu/anaconda3/envs/mcu/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-10-10 21:13:11.303895: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-10-10 21:13:11.303908: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-10-10 21:13:11.304041: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpkahogt2a\n",
      "2024-10-10 21:13:11.305536: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-10-10 21:13:11.305541: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpkahogt2a\n",
      "2024-10-10 21:13:11.309571: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-10-10 21:13:11.708374: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/hb/fpmv_swd6pb2nt84zqhq62fh0000gq/T/tmpkahogt2a\n",
      "2024-10-10 21:13:11.721360: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 417320 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the pruned, stripped, and quantized VGG16 model as vgg16_pruned_stripped_quantized_int8.tflite\n",
      "Stripped Pruned Model Size: 527.86 MB\n",
      "Quantized Model Size: 132.09 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Step 1: Load the Pre-trained VGG16 Model\n",
    "base_model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Pruning the Model with 70% Sparsity\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0, final_sparsity=0.7, begin_step=0, end_step=1000)\n",
    "}\n",
    "\n",
    "# Apply pruning to the entire model\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(base_model, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "pruned_model.summary()\n",
    "\n",
    "# Step 3: Strip pruning wrappers from the model\n",
    "stripped_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Save the stripped model without pruning wrappers\n",
    "stripped_model_filename = \"vgg16_pruned_stripped.h5\"\n",
    "stripped_model.save(stripped_model_filename)\n",
    "print(f\"Stripped pruned model saved as {stripped_model_filename}\")\n",
    "\n",
    "# Step 4: Convert the Stripped Model to a TFLite Model with Quantization\n",
    "# Load the stripped model again to ensure it's ready for conversion\n",
    "loaded_model = tf.keras.models.load_model(stripped_model_filename, compile=False)\n",
    "\n",
    "# Define a representative dataset function for int8 quantization\n",
    "def representative_data_gen():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
    "        yield [data]\n",
    "\n",
    "# Convert the stripped model to TFLite format with int8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # Quantize inputs to int8\n",
    "converter.inference_output_type = tf.int8  # Quantize outputs to int8\n",
    "\n",
    "# Convert the stripped model to a TFLite quantized model\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "# Save the quantized TFLite model\n",
    "tflite_filename = \"vgg16_pruned_stripped_quantized_int8.tflite\"\n",
    "with open(tflite_filename, \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "print(f\"Saved the pruned, stripped, and quantized VGG16 model as {tflite_filename}\")\n",
    "\n",
    "# Step 5: Print Model Size Reduction\n",
    "stripped_size = os.path.getsize(stripped_model_filename) / (1024 * 1024)\n",
    "quantized_size = os.path.getsize(tflite_filename) / (1024 * 1024)\n",
    "print(f\"Stripped Pruned Model Size: {stripped_size:.2f} MB\")\n",
    "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Data Type: <class 'numpy.int8'>\n",
      "Expected Input Shape: [  1 224 224   3]\n",
      "First 10 Ground Truth Labels: [0, 0, 0, 1, 1, 1, 2, 2, 2, 2]\n",
      "Image: ILSVRC2012_val_00009111.JPEG, Top-1 Prediction: 999, Ground Truth: 0\n",
      "Top-5 Predictions: [999 431 669 794 700]\n",
      "Image: ILSVRC2012_val_00030740.JPEG, Top-1 Prediction: 999, Ground Truth: 0\n",
      "Top-5 Predictions: [999 700 669 794 431]\n",
      "Image: ILSVRC2012_val_00046252.JPEG, Top-1 Prediction: 794, Ground Truth: 0\n",
      "Top-5 Predictions: [794 669 999   6 419]\n",
      "Image: ILSVRC2012_val_00000994.JPEG, Top-1 Prediction: 669, Ground Truth: 1\n",
      "Top-5 Predictions: [669 999 794  29   6]\n",
      "Image: ILSVRC2012_val_00002241.JPEG, Top-1 Prediction: 669, Ground Truth: 1\n",
      "Top-5 Predictions: [669 794 999 712  58]\n",
      "Image: ILSVRC2012_val_00020822.JPEG, Top-1 Prediction: 669, Ground Truth: 1\n",
      "Top-5 Predictions: [669 794 999 431  29]\n",
      "Image: ILSVRC2012_val_00014467.JPEG, Top-1 Prediction: 999, Ground Truth: 2\n",
      "Top-5 Predictions: [999 669 794 700 549]\n",
      "Image: ILSVRC2012_val_00019409.JPEG, Top-1 Prediction: 999, Ground Truth: 2\n",
      "Top-5 Predictions: [999 549 669 794 921]\n",
      "Image: ILSVRC2012_val_00027514.JPEG, Top-1 Prediction: 999, Ground Truth: 2\n",
      "Top-5 Predictions: [999 669 794 549  75]\n",
      "Image: ILSVRC2012_val_00030127.JPEG, Top-1 Prediction: 999, Ground Truth: 2\n",
      "Top-5 Predictions: [999 794 669 549 728]\n",
      "Total Images Evaluated: 50\n",
      "Top-1 Accuracy: 0.0000\n",
      "Top-5 Accuracy: 0.0200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Paths to your dataset and necessary files\n",
    "base_dir = \"/Users/xiaoguang_guo@mines.edu/Documents/projects/datasets/imagenet-mini\"\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "ground_truth_file = os.path.join(base_dir, \"imagenet_mini_gt.txt\")\n",
    "\n",
    "# Load the TFLite model\n",
    "tflite_model_path = \"vgg16_pruned_stripped_quantized_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Print the expected input data type and shape\n",
    "print(f\"Expected Input Data Type: {input_details[0]['dtype']}\")  # Should print `int8`\n",
    "print(f\"Expected Input Shape: {input_details[0]['shape']}\")     # Should print `[1, 224, 224, 3]`\n",
    "\n",
    "# Load the ground truth labels\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    ground_truth_labels = f.readlines()\n",
    "    ground_truth_labels = [int(label.strip()) for label in ground_truth_labels]\n",
    "\n",
    "# Debug: Print first few ground truth labels to ensure they are correctly loaded\n",
    "print(f\"First 10 Ground Truth Labels: {ground_truth_labels[:10]}\")\n",
    "\n",
    "# Initialize counters for Top-1 and Top-5 accuracy\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total_images = 0\n",
    "\n",
    "# Set the limit for evaluation\n",
    "max_images = 50  # Adjust this to evaluate more or fewer images\n",
    "\n",
    "# Loop through the validation set and evaluate\n",
    "for folder in sorted(os.listdir(val_dir)):\n",
    "    folder_path = os.path.join(val_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img_name in sorted(os.listdir(folder_path)):\n",
    "            if total_images >= max_images:\n",
    "                break\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            # Preprocess the image to match the TFLite model's input requirements (INT8)\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            x = image.img_to_array(img)\n",
    "\n",
    "            # Scale the input image to the appropriate range expected by the quantized model\n",
    "            # The quantized model expects input in range [-128, 127] when using INT8 precision\n",
    "            # Adjust scaling to match this expectation\n",
    "            x = (x - 128).astype(np.int8)  # Scale the image and convert to int8\n",
    "\n",
    "            # Expand dimensions to match model input and set tensor\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            \n",
    "            # Assign the preprocessed image to the TFLite model's input tensor\n",
    "            interpreter.set_tensor(input_details[0]['index'], x)\n",
    "\n",
    "            # Run inference using the TFLite model\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Retrieve the predictions from the output tensor\n",
    "            preds = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "            # Decode the top-5 predictions\n",
    "            top5_preds = np.argsort(preds)[-5:][::-1]  # Get top-5 indices\n",
    "            top1_pred = top5_preds[0]  # Top-1 prediction\n",
    "\n",
    "            # Get the corresponding ground truth label\n",
    "            ground_truth_label = ground_truth_labels[total_images]\n",
    "            total_images += 1\n",
    "\n",
    "            # Debug: Print predictions and ground truth for the first few images\n",
    "            if total_images <= 10:\n",
    "                print(f\"Image: {img_name}, Top-1 Prediction: {top1_pred}, Ground Truth: {ground_truth_label}\")\n",
    "                print(f\"Top-5 Predictions: {top5_preds}\")\n",
    "\n",
    "            # Check if the top-1 prediction matches the ground truth\n",
    "            if top1_pred == ground_truth_label:\n",
    "                top1_correct += 1\n",
    "\n",
    "            # Check if ground truth label is in top-5 predictions\n",
    "            if ground_truth_label in top5_preds:\n",
    "                top5_correct += 1\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "top1_accuracy = top1_correct / total_images\n",
    "top5_accuracy = top5_correct / total_images\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Total Images Evaluated: {total_images}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguang_guo@mines.edu/anaconda3/envs/mcu/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved as vgg16_pruned.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "\n",
    "# Prune the model\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.0, final_sparsity=0.7, begin_step=0, end_step=100)\n",
    "}\n",
    "pruned_model = sparsity.prune_low_magnitude(vgg16_model, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model_filename = \"vgg16_pruned.h5\"\n",
    "pruned_model.save(pruned_model_filename)\n",
    "print(f\"Pruned model saved as {pruned_model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prune_low_magnitude() missing 1 required positional argument: 'to_prune'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define a custom object scope to correctly register the PruneLowMagnitude layer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcustom_object_scope({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPruneLowMagnitude\u001b[39m\u001b[38;5;124m'\u001b[39m: prune_low_magnitude}):\n\u001b[0;32m----> 6\u001b[0m     pruned_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruned_model_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Continue with the quantization and TFLite conversion process\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentative_data_gen\u001b[39m():\n",
      "File \u001b[0;32m~/anaconda3/envs/mcu/lib/python3.8/site-packages/keras/src/saving/saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    231\u001b[0m         filepath,\n\u001b[1;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mcu/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/mcu/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/anaconda3/envs/mcu/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mTypeError\u001b[0m: prune_low_magnitude() missing 1 required positional argument: 'to_prune'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude\n",
    "\n",
    "# Define a custom object scope to correctly register the PruneLowMagnitude layer\n",
    "with tf.keras.utils.custom_object_scope({'PruneLowMagnitude': prune_low_magnitude}):\n",
    "    pruned_model = tf.keras.models.load_model(pruned_model_filename, compile=False)\n",
    "\n",
    "# Continue with the quantization and TFLite conversion process\n",
    "def representative_data_gen():\n",
    "    for _ in range(100):\n",
    "        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n",
    "\n",
    "# Convert the pruned model to TFLite format with int8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "# Save the quantized TFLite model\n",
    "tflite_model_filename = \"vgg16_pruned_quantized_fixed.tflite\"\n",
    "with open(tflite_model_filename, \"wb\") as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "print(f\"Quantized model saved as {tflite_model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
